# üì§ Upload Flow - Development vs Production

Guida completa al flusso di caricamento file e storage dei dati in MagiXcel.

---

## üéØ Overview: Cosa Viene Salvato Dove?

### Storage Strategy

| Tipo Dato | Dove Viene Salvato | Development | Production |
|-----------|-------------------|-------------|------------|
| **Metadata** (sessione, workspace, views) | **Database** | SQLite | Supabase (PostgreSQL) |
| **File Originale** (.xlsx, .csv) | **Storage** | Local Filesystem | Cloudflare R2 |
| **File Parquet** (query-optimized) | **Storage** | Local Filesystem | Cloudflare R2 |
| **Query Results** (cache) | **Cache** | Memory | Redis/Vercel KV |
| **Data Rows** | ‚ö†Ô∏è **MAI nel DB** | Parquet only | Parquet only |

**Principio Chiave**:
- üìä **Dati RAW** (righe Excel/CSV) ‚Üí Solo in **Parquet** (storage)
- üóÇÔ∏è **Metadata** (nome file, # righe, colonne) ‚Üí Solo in **Database**
- ‚ö° **Cache** (risultati query) ‚Üí Solo in **Cache layer**

---

## üîÑ Development Flow (Local)

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Browser   ‚îÇ Upload file.xlsx (1MB)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ POST /api/upload
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Next.js API Route                 ‚îÇ
‚îÇ   (app/api/upload/route.ts)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îú‚îÄ 1) Save to /tmp/
       ‚îú‚îÄ 2) DuckDB converts to Parquet
       ‚îú‚îÄ 3) Save files to local storage
       ‚îú‚îÄ 4) Save metadata to SQLite
       ‚îî‚îÄ 5) Return sessionId

       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LOCAL STORAGE (./data/files/)       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ {sessionId}/                    ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ original.xlsx  (1.0 MB)     ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet   (0.3 MB) ‚úÖ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SQLITE (./data/magixcel.db)         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ workspaces                      ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ sessions  ‚Üê metadata salvato    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ views                           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ active_views                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Step-by-Step: Upload in Development

**Step 1: Frontend Upload**
```typescript
// Frontend (components/upload/FileUploader.tsx)
const formData = new FormData();
formData.append('file', selectedFile);         // file.xlsx (1MB)
formData.append('sessionName', 'My Data');
formData.append('workspaceId', 'default');

const response = await fetch('/api/upload', {
  method: 'POST',
  body: formData
});
```

**Step 2: Backend Riceve File**
```typescript
// Backend (app/api/upload/route.ts)
const file = formData.get('file');  // File object
const buffer = Buffer.from(await file.arrayBuffer());

// Save to temporary directory
const tempFilePath = '/tmp/magixcel-uploads/1234567-file.xlsx';
fs.writeFileSync(tempFilePath, buffer);
```

**Step 3: DuckDB Conversion**
```typescript
// DuckDB converts Excel ‚Üí Parquet
const converter = createParquetConverter();
const result = await converter.convert({
  inputPath: tempFilePath,                    // /tmp/1234567-file.xlsx
  outputPath: '/tmp/1234567-data.parquet',    // Temp Parquet
  fileType: 'xlsx'
});

// Result:
// {
//   metadata: {
//     sheets: [{ name: 'Sheet1', rowCount: 1000, columns: [...] }],
//     totalRows: 1000,
//     totalColumns: 5,
//     parquetSize: 300000  // 0.3 MB (70% compression!)
//   },
//   fileHash: 'sha256-abc123...',
//   compressionRatio: 0.30
// }
```

**Step 4: Save to Local Storage**
```typescript
const storage = getStorageAdapter();  // Returns LocalStorageAdapter

// Upload original file
await storage.upload(
  'files/abc-123/original.xlsx',     // path
  buffer,                             // file content
  'application/vnd.openxmlformats...'
);

// Upload Parquet file
const parquetBuffer = fs.readFileSync('/tmp/1234567-data.parquet');
await storage.upload(
  'files/abc-123/data.parquet',
  parquetBuffer,
  'application/octet-stream'
);
```

**Physical Storage Location (Development)**:
```
./data/files/
‚îî‚îÄ‚îÄ abc-123/                           ‚Üê sessionId
    ‚îú‚îÄ‚îÄ original.xlsx                  ‚Üê 1.0 MB (original file)
    ‚îî‚îÄ‚îÄ data.parquet                   ‚Üê 0.3 MB (columnar, compressed)
```

**Step 5: Save Metadata to SQLite**
```typescript
const db = getDBAdapter();  // Returns SQLiteAdapter
const userId = getCurrentUserId();  // Returns 'dev-user'

const session = await db.createSession(userId, {
  workspace_id: 'default',
  name: 'My Data',
  original_file_name: 'file.xlsx',
  file_type: 'xlsx',
  file_size: 1000000,                              // 1 MB
  file_hash: 'sha256-abc123...',
  r2_path_original: 'files/abc-123/original.xlsx', // Storage path
  r2_path_parquet: 'files/abc-123/data.parquet',   // Parquet path
  metadata: {                                       // JSONB metadata
    sheets: [{
      name: 'Sheet1',
      rowCount: 1000,
      columnCount: 5,
      columns: [
        { name: 'Name', type: 'varchar' },
        { name: 'Age', type: 'integer' },
        { name: 'Email', type: 'varchar' },
        { name: 'Salary', type: 'decimal' },
        { name: 'Date', type: 'date' }
      ]
    }],
    totalRows: 1000,
    totalColumns: 5,
    parquetSize: 300000,
    compressionRatio: 0.30
  }
});
```

**Database Content (SQLite)**:
```sql
-- sessions table
INSERT INTO sessions (
  id,                    -- 'abc-123'
  workspace_id,          -- 'default'
  user_id,               -- 'dev-user'
  name,                  -- 'My Data'
  original_file_name,    -- 'file.xlsx'
  file_type,             -- 'xlsx'
  file_size,             -- 1000000
  file_hash,             -- 'sha256-abc123...'
  r2_path_original,      -- 'files/abc-123/original.xlsx'
  r2_path_parquet,       -- 'files/abc-123/data.parquet'
  metadata,              -- '{"sheets":[{"name":"Sheet1",...}],...}'
  created_at,            -- '2025-01-24 14:00:00'
  updated_at,            -- '2025-01-24 14:00:00'
  last_accessed_at       -- '2025-01-24 14:00:00'
);
```

**Step 6: Cleanup & Response**
```typescript
// Delete temp files
fs.unlinkSync(tempFilePath);
fs.unlinkSync(tempParquetPath);

// Return success
return {
  success: true,
  data: {
    sessionId: 'abc-123',
    sheets: [...],
    totalRows: 1000,
    totalColumns: 5,
    compressionRatio: 0.30
  }
};
```

---

## üåê Production Flow (Cloud)

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Browser   ‚îÇ Upload file.xlsx (1MB)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ POST /api/upload
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Vercel Edge Function              ‚îÇ
‚îÇ   (app/api/upload/route.ts)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îú‚îÄ 1) Save to /tmp/ (ephemeral)
       ‚îú‚îÄ 2) DuckDB converts to Parquet
       ‚îú‚îÄ 3) Upload to Cloudflare R2
       ‚îú‚îÄ 4) Save metadata to Supabase
       ‚îî‚îÄ 5) Return sessionId

       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CLOUDFLARE R2 (Object Storage)      ‚îÇ
‚îÇ  https://r2.cloudflare.com/bucket/   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ files/abc-123/                  ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ original.xlsx  (1.0 MB)     ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet   (0.3 MB) ‚úÖ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SUPABASE (PostgreSQL)               ‚îÇ
‚îÇ  https://xyz.supabase.co             ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ workspaces                      ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ sessions  ‚Üê metadata (JSONB)    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ views                           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ active_views                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  VERCEL KV (Redis Cache)             ‚îÇ
‚îÇ  filter:abc-123:hash123 ‚Üí results    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Differences in Production

**1. Storage Adapter**
```typescript
// Factory selects based on env
const storage = getStorageAdapter();
// Returns: R2StorageAdapter (instead of LocalStorageAdapter)
```

**2. Database Adapter**
```typescript
const db = getDBAdapter();
// Returns: SupabaseAdapter (instead of SQLiteAdapter)
```

**3. File Upload to R2**
```typescript
// R2 (S3-compatible API)
await storage.upload(
  'files/abc-123/original.xlsx',
  buffer,
  'application/vnd.openxmlformats...'
);

// Physical location:
// https://pub-xxxxx.r2.dev/files/abc-123/original.xlsx
```

**4. Metadata to Supabase**
```sql
-- PostgreSQL (Supabase)
INSERT INTO sessions (
  id,                    -- UUID
  workspace_id,          -- UUID (FK)
  user_id,               -- UUID (FK to auth.users)
  name,                  -- TEXT
  metadata,              -- JSONB (native JSON type!)
  r2_path_parquet,       -- TEXT (R2 URL)
  ...
);
```

**5. User Authentication**
```typescript
// In production, getCurrentUserId() gets from:
// - Supabase Auth (JWT token)
// - Session cookie
// - Authentication header

const userId = getCurrentUserId();
// Returns: actual user UUID (not 'dev-user')
```

---

## üìä Data Querying Flow

### Development: Reading Data

**Step 1: User Opens Session**
```typescript
// Frontend
const response = await fetch(`/api/session/${sessionId}/data`);
```

**Step 2: Backend Gets Parquet Path**
```typescript
// app/api/session/[id]/data/route.ts
const db = getDBAdapter();
const session = await db.getSession(sessionId, userId);

// Session contains:
// {
//   r2_path_parquet: 'files/abc-123/data.parquet',
//   metadata: { sheets: [...], totalRows: 1000, ... }
// }
```

**Step 3: Get Absolute File Path (Development)**
```typescript
const storage = getStorageAdapter();  // LocalStorageAdapter

const parquetPath = storage.getAbsolutePath(session.r2_path_parquet);
// Returns: '/Users/you/Progetti/magixcel/data/files/abc-123/data.parquet'
```

**Step 4: DuckDB Queries Parquet**
```typescript
const duckdb = createDuckDBClient();
await duckdb.connect();

const query = `SELECT * FROM read_parquet('${parquetPath}') LIMIT 10000`;
const result = await duckdb.query(query);

// Result:
// {
//   rows: [
//     { Name: 'John', Age: 30, Email: 'john@...', ... },
//     { Name: 'Jane', Age: 25, Email: 'jane@...', ... },
//     // ... 10000 rows
//   ],
//   rowCount: 10000,
//   columns: ['Name', 'Age', 'Email', 'Salary', 'Date'],
//   executionTime: 45  // milliseconds!
// }
```

**Step 5: Return to Frontend**
```typescript
return {
  success: true,
  data: result.rows,
  sheets: session.metadata.sheets,
  metadata: {
    rowCount: 10000,
    totalRows: 1000000,
    executionTime: 45
  }
};
```

### Production: Reading Data

**Difference: R2 Public URL**
```typescript
// In production
const parquetPath = await storage.getPublicUrl(session.r2_path_parquet);
// Returns: 'https://pub-xxxxx.r2.dev/files/abc-123/data.parquet'

// DuckDB can read from HTTP URL!
const query = `SELECT * FROM read_parquet('${parquetPath}') LIMIT 10000`;
```

**Note**: DuckDB supports reading Parquet files from:
- ‚úÖ Local filesystem (development)
- ‚úÖ HTTP URLs (production with R2)
- ‚úÖ S3 URLs (with credentials)

---

## üîç Filtering Flow

### Step 1: User Applies Filters

```typescript
// Frontend
const filters = [
  { column: 'Age', operator: 'greaterThan', value: 25 },
  { column: 'Salary', operator: 'greaterThan', value: 50000 }
];

const response = await fetch('/api/filter', {
  method: 'POST',
  body: JSON.stringify({
    sessionId: 'abc-123',
    filters,
    combinator: 'AND',
    pagination: { page: 1, pageSize: 100 }
  })
});
```

### Step 2: Check Cache

```typescript
// app/api/filter/route.ts
const cache = getCacheAdapter();  // Memory (dev) or Redis (prod)

const filterHash = JSON.stringify({ filters, combinator });
const cacheKey = `filter:abc-123:${hash}:page-1`;

const cached = await cache.get(cacheKey);
if (cached) {
  return { success: true, data: cached, cached: true };
}
```

### Step 3: Build SQL with Query Builder

```typescript
const queryBuilder = createQueryBuilder({
  sessionId: 'abc-123',
  sheetName: 'Sheet1',
  parquetPath: '/path/to/data.parquet'
});

const sql = queryBuilder.buildQuery({
  filters,
  combinator: 'AND',
  pagination: { page: 1, pageSize: 100 }
});

// Generated SQL:
// SELECT *
// FROM read_parquet('/path/to/data.parquet')
// WHERE (Age > 25 AND Salary > 50000)
// LIMIT 100 OFFSET 0
```

### Step 4: Execute with DuckDB

```typescript
const duckdb = createDuckDBClient();
await duckdb.connect();

const result = await duckdb.query(sql);
// Execution time: ~50ms for 1M rows!

await duckdb.close();
```

### Step 5: Cache Result

```typescript
await cache.set(cacheKey, result, 3600);  // TTL: 1 hour

return {
  success: true,
  data: result.rows,
  metadata: {
    rowCount: result.rowCount,
    executionTime: result.executionTime,
    cached: false
  }
};
```

---

## üóÇÔ∏è Database Schema Deep Dive

### What's Stored in Database (Metadata ONLY)

**Workspaces Table**
```sql
CREATE TABLE workspaces (
  id TEXT PRIMARY KEY,              -- 'default', 'workspace-123'
  user_id TEXT NOT NULL,            -- 'dev-user' (dev) or UUID (prod)
  name TEXT NOT NULL,               -- 'My Workspace'
  description TEXT,                 -- 'Project data files'
  color TEXT DEFAULT '#3B82F6',     -- UI color
  icon TEXT DEFAULT 'folder',       -- UI icon
  is_default INTEGER DEFAULT 0,     -- Boolean (0/1)
  created_at TEXT,
  updated_at TEXT
);
```

**Sessions Table** (File Metadata)
```sql
CREATE TABLE sessions (
  id TEXT PRIMARY KEY,                    -- 'abc-123'
  workspace_id TEXT NOT NULL,             -- FK to workspaces
  user_id TEXT NOT NULL,                  -- 'dev-user' or UUID

  -- File Info
  name TEXT NOT NULL,                     -- 'Sales Data 2024'
  original_file_name TEXT NOT NULL,       -- 'sales_2024.xlsx'
  file_type TEXT NOT NULL,                -- 'xlsx', 'csv'
  file_size INTEGER NOT NULL,             -- 1000000 (bytes)
  file_hash TEXT NOT NULL,                -- 'sha256-abc123...'

  -- Storage Paths (NO actual data, just paths!)
  r2_path_original TEXT NOT NULL,         -- 'files/abc-123/original.xlsx'
  r2_path_parquet TEXT NOT NULL,          -- 'files/abc-123/data.parquet' ‚úÖ

  -- Metadata (JSONB)
  metadata TEXT NOT NULL,                 -- JSON: {sheets, totalRows, totalColumns}

  -- Active Filters (JSON)
  active_filters TEXT,                    -- JSON: temporary filters

  -- Timestamps
  created_at TEXT,
  updated_at TEXT,
  last_accessed_at TEXT,
  deleted_at TEXT,                        -- Soft delete

  FOREIGN KEY (workspace_id) REFERENCES workspaces(id)
);
```

**Example Session Row**:
```json
{
  "id": "abc-123",
  "workspace_id": "default",
  "user_id": "dev-user",
  "name": "Sales Data 2024",
  "original_file_name": "sales_2024.xlsx",
  "file_type": "xlsx",
  "file_size": 1000000,
  "file_hash": "sha256-abc123...",
  "r2_path_original": "files/abc-123/original.xlsx",
  "r2_path_parquet": "files/abc-123/data.parquet",
  "metadata": {
    "sheets": [
      {
        "name": "Sheet1",
        "rowCount": 1000,
        "columnCount": 5,
        "columns": [
          { "name": "Name", "type": "varchar" },
          { "name": "Age", "type": "integer" },
          { "name": "Email", "type": "varchar" },
          { "name": "Salary", "type": "decimal" },
          { "name": "Date", "type": "date" }
        ]
      }
    ],
    "totalRows": 1000,
    "totalColumns": 5,
    "parquetSize": 300000,
    "compressionRatio": 0.30
  },
  "active_filters": null,
  "created_at": "2025-01-24T14:00:00Z",
  "updated_at": "2025-01-24T14:00:00Z",
  "last_accessed_at": "2025-01-24T14:30:00Z",
  "deleted_at": null
}
```

**Views Table** (Saved Filter Configurations)
```sql
CREATE TABLE views (
  id TEXT PRIMARY KEY,
  workspace_id TEXT NOT NULL,       -- Views are global to workspace
  session_id TEXT NOT NULL,         -- Which file this view is for
  user_id TEXT NOT NULL,

  name TEXT NOT NULL,               -- 'High Earners'
  description TEXT,                 -- 'Employees earning > $50k'
  category TEXT DEFAULT 'Custom',

  filter_config TEXT NOT NULL,      -- JSON: {filters, combinator}

  view_type TEXT DEFAULT 'filters_only',
  snapshot_data TEXT,               -- JSON array (if snapshot)

  is_public INTEGER DEFAULT 0,
  public_link_id TEXT UNIQUE,

  created_at TEXT,
  updated_at TEXT,
  access_count INTEGER DEFAULT 0
);
```

**Example View Row**:
```json
{
  "id": "view-456",
  "workspace_id": "default",
  "session_id": "abc-123",
  "user_id": "dev-user",
  "name": "High Earners",
  "description": "Employees earning > $50k",
  "category": "HR",
  "filter_config": {
    "filters": [
      { "column": "Salary", "operator": "greaterThan", "value": 50000 }
    ],
    "combinator": "AND"
  },
  "view_type": "filters_only",
  "is_public": 0,
  "created_at": "2025-01-24T14:15:00Z",
  "access_count": 5
}
```

---

## üéØ Summary: Development vs Production

### Development

| Component | Technology | Location |
|-----------|-----------|----------|
| **Database** | SQLite | `./data/magixcel.db` |
| **File Storage** | Local Filesystem | `./data/files/{sessionId}/` |
| **Original Files** | Local | `./data/files/{id}/original.xlsx` |
| **Parquet Files** | Local | `./data/files/{id}/data.parquet` |
| **Cache** | Memory (in-process) | RAM |
| **User ID** | Hardcoded | `'dev-user'` |
| **Authentication** | None | Not needed |

**Environment**:
```env
NODE_ENV=development
DB_PROVIDER=sqlite
STORAGE_PROVIDER=local
CACHE_PROVIDER=memory
DEV_USER_ID=dev-user
```

### Production

| Component | Technology | Location |
|-----------|-----------|----------|
| **Database** | Supabase (PostgreSQL) | Cloud (managed) |
| **File Storage** | Cloudflare R2 | Cloud (S3-compatible) |
| **Original Files** | R2 | `https://r2.../files/{id}/original.xlsx` |
| **Parquet Files** | R2 | `https://r2.../files/{id}/data.parquet` |
| **Cache** | Vercel KV (Redis) | Cloud (managed) |
| **User ID** | Supabase Auth | UUID from JWT token |
| **Authentication** | Supabase Auth | Email/Password, OAuth |

**Environment**:
```env
NODE_ENV=production
DB_PROVIDER=supabase
SUPABASE_URL=https://xyz.supabase.co
SUPABASE_ANON_KEY=eyJhbGc...

STORAGE_PROVIDER=r2
R2_ACCOUNT_ID=xxx
R2_ACCESS_KEY_ID=xxx
R2_SECRET_ACCESS_KEY=xxx
R2_BUCKET_NAME=magixcel-files

CACHE_PROVIDER=redis
REDIS_URL=redis://default:xxx@...
```

---

## üìà Performance Metrics

### File Upload

| Operation | Development | Production |
|-----------|-------------|------------|
| Upload 1MB file | ~3s | ~4s (network) |
| DuckDB conversion | ~1s | ~1s |
| Parquet compression | 60-70% | 60-70% |
| Storage write | ~100ms (local) | ~500ms (R2) |
| DB metadata write | ~10ms (SQLite) | ~50ms (Supabase) |

### Data Querying

| Dataset Size | Query Time (DuckDB) | Traditional (JSON) |
|--------------|---------------------|-------------------|
| 10k rows | ~20ms | ~500ms |
| 100k rows | ~50ms | ~5s |
| 1M rows | ~100ms | ~50s |
| 10M rows | ~500ms | Out of Memory ‚ùå |

### Storage Size

| File Type | Original | Parquet | Compression |
|-----------|----------|---------|-------------|
| CSV (100k rows) | 10 MB | 3 MB | **70%** |
| Excel (100k rows) | 15 MB | 4 MB | **73%** |
| Excel (1M rows) | 150 MB | 45 MB | **70%** |

---

## üöÄ Next Steps

**Test the Flow**:
1. Upload a file ‚Üí Check `./data/files/{id}/` for Parquet
2. View data ‚Üí DuckDB queries Parquet in <100ms
3. Apply filters ‚Üí Results cached for 1h
4. Create view ‚Üí Filter config saved to DB

**When Moving to Production**:
1. Set environment variables for Supabase, R2, Redis
2. Same code works (Adapter Pattern handles the switch!)
3. Users get authentication via Supabase Auth
4. Files stored in R2 instead of local filesystem

---

**Created**: 2025-01-24
**Last Updated**: 2025-01-24
